{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "import pandas as pd\r\n",
    "import os\r\n",
    "from tqdm import tqdm\r\n",
    "from dateutil.relativedelta import *\r\n",
    "from datetime import datetime as dt\r\n",
    "\r\n",
    "date_index_reference = 'issue_sheet_index.csv'\r\n",
    "# chunksize = 1000\r\n",
    "date_format = '%d-%b-%y'\r\n",
    "custom_date_parser = lambda x: dt.strptime(x, date_format)\r\n",
    "\r\n",
    "if not os.path.isfile(date_index_reference):\r\n",
    "    print('date index needs to be instantiated, proceeding to do that')\r\n",
    "    #Initialize the index df\r\n",
    "    index_df = pd.DataFrame([], columns=['date_issue', 'sheet_path', 'sheet_num'])\r\n",
    "\r\n",
    "    formatted_csv_path = '../00_The_Economist_Scraper/formatted_csvs/'\r\n",
    "    formatted_csv_stub = \"formatted_sub_article_text\"\r\n",
    "    suffix = '.csv'\r\n",
    "\r\n",
    "    # GOING THROUGH EACH FORMATTED CSV\r\n",
    "    for sub_csv_index in tqdm(range(0,124)):\r\n",
    "        read_path = formatted_csv_path + formatted_csv_stub + str(sub_csv_index) + suffix\r\n",
    "        temp_df =  pd.read_csv(read_path, parse_dates=['date'], date_parser=custom_date_parser)\r\n",
    "\r\n",
    "        # Identifying unique dates\r\n",
    "        temp_df = temp_df.drop_duplicates(subset=['date'], keep='first')\r\n",
    "        temp_df = temp_df['date']\r\n",
    "\r\n",
    "        temp_index = pd.DataFrame([], columns=['date_issue', 'sheet_path', 'sheet_num'])\r\n",
    "\r\n",
    "        # Pulling out the info of interest \r\n",
    "        for issue_row in temp_df.index.tolist():\r\n",
    "            row_obs = temp_df.loc[issue_row]\r\n",
    "            # recall that temp_df is now just a single column of 'date\r\n",
    "            temp_index.loc[issue_row, 'date_issue'] = temp_df.loc[issue_row].date()\r\n",
    "            temp_index.loc[issue_row, 'sheet_path'] = formatted_csv_path + formatted_csv_stub + str(sub_csv_index) + suffix\r\n",
    "            temp_index.loc[issue_row, 'sheet_num'] = sub_csv_index\r\n",
    "        ### REMEMBER THAT A DATE MIGHT APPEAR IN/CROSS OVER MORE THAN ONE SHEET\r\n",
    "        # Need to concat and reset indices because first instances of different issues may appear on the same row (i.e. \"index\" of temp_df) of different sheets. Need to do this to avoid conflicts\r\n",
    "        index_df = pd.concat([index_df, temp_index], ignore_index=True)\r\n",
    "        index_df.reset_index(drop=True, inplace=True)\r\n",
    "        index_df['date_issue'] = pd.to_datetime(index_df['date_issue'])\r\n",
    "\r\n",
    "    index_df.to_csv(date_index_reference, index=False)\r\n",
    "        \r\n",
    "else:\r\n",
    "    print('Issues are already indexed for their dates, carry on')\r\n",
    "    index_df = pd.read_csv(date_index_reference)\r\n",
    "\r\n",
    "print(index_df.head(5))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "date index needs to be instantiated, proceeding to do that\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 124/124 [00:11<00:00, 10.88it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "  date_issue                                         sheet_path sheet_num\n",
      "0 1992-01-04  ../00_The_Economist_Scraper/formatted_csvs/for...         0\n",
      "1 1992-01-11  ../00_The_Economist_Scraper/formatted_csvs/for...         0\n",
      "2 1992-02-15  ../00_The_Economist_Scraper/formatted_csvs/for...         0\n",
      "3 1992-02-22  ../00_The_Economist_Scraper/formatted_csvs/for...         0\n",
      "4 1992-02-29  ../00_The_Economist_Scraper/formatted_csvs/for...         0\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "### READING IN LEADER TERM DATA\r\n",
    "leader_term_name_df = pd.read_csv('all_leaders_econ_styling.csv', encoding='latin1')\r\n",
    "leader_term_name_df = leader_term_name_df.drop_duplicates(subset='leadid')\r\n",
    "\r\n",
    "\r\n",
    "leader_term_name_df['term_start'] = dt.now\r\n",
    "leader_term_name_df['term_end'] =dt.now\r\n",
    "\r\n",
    "# leader_term_name_df\r\n",
    "for index in tqdm(leader_term_name_df.index.tolist()):\r\n",
    "    # FORMAT: start = dt(year, month, day)\r\n",
    "    leader_term_name_df.loc[index, 'term_start'] = dt(leader_term_name_df.loc[index, 'start_year'], leader_term_name_df.loc[index, 'start_month'], leader_term_name_df.loc[index, 'start_date'])\r\n",
    "    \r\n",
    "    # Push to the end of the month as far as possible (accounting for leap years)\r\n",
    "    # Note that we are pushing the end of the term out 6 months anyway (in case we miss an issue printed on the 29th-31st)\r\n",
    "    leader_term_name_df.loc[index, 'term_end'] = dt(leader_term_name_df.loc[index, 'end_year'], leader_term_name_df.loc[index, 'end_month'], 28)\r\n",
    "\r\n",
    "print(leader_term_name_df.head(5))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 560/560 [00:00<00:00, 2082.30it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "    ccode  country  year     leadid  polity  start_date  start_month  \\\n",
      "0     339  Albania  2002  A2.9-4924       7          29           10   \n",
      "1     339  Albania  2002  A2.9-4927       7          22            2   \n",
      "2     339  Albania  2002  A2.9-4930       7          31            7   \n",
      "6     339  Albania  2005    A3.0-79       9          11            9   \n",
      "15    339  Albania  2013   cb-339-1       9          15            9   \n",
      "\n",
      "    start_year  end_month  end_year  gender econ_style_first econ_style_last  \\\n",
      "0         1999          2      2002       1             ilir            meta   \n",
      "1         2002          7      2002       1          pandeli           majko   \n",
      "2         2002          9      2005       1            fatos            nano   \n",
      "6         2005          9      2013       1             sali         berisha   \n",
      "15        2013          8      2021       1              edi            rama   \n",
      "\n",
      "   econ_style_alias      leader           term_start             term_end  \n",
      "0               NaN        Meta  1999-10-29 00:00:00  2002-02-28 00:00:00  \n",
      "1               NaN       Majko  2002-02-22 00:00:00  2002-07-28 00:00:00  \n",
      "2               NaN  Fatos Nano  2002-07-31 00:00:00  2005-09-28 00:00:00  \n",
      "6               NaN     Berisha  2005-09-11 00:00:00  2013-09-28 00:00:00  \n",
      "15              NaN        Rama  2013-09-15 00:00:00  2021-08-28 00:00:00  \n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "source": [
    "# MATCHING TERM-WINDOWS (term length +/- 6 months) WITH ECONOMIST ISSUE DATES\r\n",
    "# We then pair this with the dat_index_reference.csv to figure out which chunks of formatted_compiled_articles.csv we should read\r\n",
    "six_month_margin = relativedelta(months = 6)\r\n",
    "\r\n",
    "leaders_windows_indices_df = leader_term_name_df\r\n",
    "\r\n",
    "# Constructing the 6 month window around the term start\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.assign(leader_aprox_starts = lambda df: df.term_start - six_month_margin)\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.assign(leader_aprox_ends = lambda df: df.term_end + six_month_margin)\r\n",
    "\r\n",
    "\r\n",
    "# Merge window-start and window-end dates with the \"nearest\" (FORWARD OR BACKWARD) Economist issue date.\r\n",
    "# Read in that issue's starting row/index\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.sort_values(by=['leader_aprox_starts'])\r\n",
    "leaders_windows_indices_df['leader_aprox_starts'] = pd.to_datetime(leaders_windows_indices_df['leader_aprox_starts'])\r\n",
    "\r\n",
    "\r\n",
    "leaders_windows_indices_df = pd.merge_asof(left=leaders_windows_indices_df, right=index_df, left_on='leader_aprox_starts', right_on='date_issue', direction='nearest')\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.rename(columns={'date_issue' : 'date_start_issue', 'sheet_path' : 'start_sheet_path', 'sheet_num' : 'start_sheet_num'})\r\n",
    "\r\n",
    "################################################\r\n",
    "# Merge window-start and window-end dates with the \"nearest\" (FORWARD OR BACKWARD) Economist issue date.\r\n",
    "# Read in that issue's ending row/index\r\n",
    "################################################\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.sort_values(by=['leader_aprox_ends'])\r\n",
    "leaders_windows_indices_df = pd.merge_asof(left=leaders_windows_indices_df, right=index_df, left_on='leader_aprox_ends', right_on='date_issue', direction='nearest')\r\n",
    "\r\n",
    "\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.rename(columns={'date_issue': 'date_end_issue', 'sheet_path': 'end_sheet_path', 'sheet_num' : 'end_sheet_num'})\r\n",
    "\r\n",
    "\r\n",
    "\r\n",
    "################################################\r\n",
    "# Generating number of rows that need to be read-in from the formatted_compiled_articles.csv file\r\n",
    "################################################\r\n",
    "# leaders_windows_indices_df = leaders_windows_indices_df.assign(nrows = lambda df: df.issue_ending_row - df.issue_starting_row - 1)\r\n",
    "\r\n",
    "\r\n",
    "################################################\r\n",
    "# Add in titles and adjectives\r\n",
    "################################################\r\n",
    "adjectives_df = pd.read_csv('national_titles_adjectives.csv').drop(columns=['country'])\r\n",
    "leaders_windows_indices_df = pd.merge(leaders_windows_indices_df, adjectives_df, how='left', on='ccode')\r\n",
    "\r\n",
    "\r\n",
    "print(len(leader_term_name_df))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "560\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "eee4b454d9c886d42d598d46356540012445065f4eb18fec34c9038948f94cf2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}