{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "source": [
                "import pandas as pd\r\n",
                "from allennlp.predictors.predictor import Predictor\r\n",
                "from datetime import datetime as dt\r\n",
                "from dateutil.relativedelta import *\r\n",
                "from tqdm import tqdm\r\n",
                "from fuzzywuzzy import fuzz\r\n",
                "from fuzzywuzzy import process\r\n",
                "import os\r\n",
                "import allennlp_models.tagging\r\n",
                "import spacy\r\n",
                "import re\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "source": [
                "date_index_reference = 'date_index_reference.csv'\r\n",
                "chunksize = 1000\r\n",
                "date_format = '%d-%b-%y'\r\n",
                "\r\n",
                "\r\n",
                "custom_date_parser = lambda x: dt.strptime(x, date_format)\r\n",
                "\r\n",
                "## DETERMINING IF WE NEED TO CRAETE A DATE-INDEX\r\n",
                "\r\n",
                "if not os.path.isfile(date_index_reference):\r\n",
                "    print('dates need to be instantiated, proceeding to do that')\r\n",
                "    chunks = pd.read_csv('formatted_compiled_articles.csv', chunksize=chunksize, parse_dates =['date'], date_parser=custom_date_parser)\r\n",
                "    df = pd.concat(chunks)\r\n",
                "    last_row = len(df)\r\n",
                "    df = df['date']\r\n",
                "    df = df.drop_duplicates(keep='first')\r\n",
                "\r\n",
                "    date_index_df = pd.DataFrame([], columns = ['date_issue', 'issue_starting_row', 'issue_ending_row'])\r\n",
                "\r\n",
                "    for index in tqdm(df.index.tolist()):\r\n",
                "        date_index_df.loc[index, 'date_issue'] = df.loc[index]\r\n",
                "        date_index_df.loc[index, 'issue_starting_row'] = index\r\n",
                "    \r\n",
                "    date_index_df.reset_index(drop=True, inplace=True)\r\n",
                "\r\n",
                "    for index in date_index_df.index:\r\n",
                "        try:\r\n",
                "           date_index_df.loc[index, 'issue_ending_row'] = date_index_df.loc[index+1, 'issue_starting_row']\r\n",
                "        except: \r\n",
                "            date_index_df.loc[index, 'issue_ending_row'] = last_row\r\n",
                "    \r\n",
                "    date_index_df['date_issue'] = pd.to_datetime(date_index_df['date_issue'])\r\n",
                "    date_index_df.to_csv('date_index_reference.csv', index=False)\r\n",
                "else:\r\n",
                "    print('dates already indexed, carry on')\r\n",
                "    date_index_df = pd.read_csv('date_index_reference.csv', parse_dates=['date_issue'])\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stdout",
                    "text": [
                        "dates already indexed, carry on\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "source": [
                "### READING IN LEADER TERM DATA\r\n",
                "leader_term_name_df = pd.read_csv('all_leaders_econ_styling.csv', encoding='latin1')\r\n",
                "leader_term_name_df = leader_term_name_df.drop_duplicates(subset='leadid')\r\n",
                "\r\n",
                "\r\n",
                "leader_term_name_df['term_start'] = dt.now\r\n",
                "leader_term_name_df['term_end'] =dt.now\r\n",
                "\r\n",
                "# leader_term_name_df\r\n",
                "for index in tqdm(leader_term_name_df.index.tolist()):\r\n",
                "    leader_term_name_df.loc[index, 'term_start'] = dt(leader_term_name_df.loc[index, 'start_year'], leader_term_name_df.loc[index, 'start_month'], leader_term_name_df.loc[index, 'start_date'])\r\n",
                "    try:\r\n",
                "        leader_term_name_df.loc[index, 'term_end'] = dt(leader_term_name_df.loc[index, 'end_year'], leader_term_name_df.loc[index, 'end_month'], leader_term_name_df[index+1, 'start_date'])\r\n",
                "    except:\r\n",
                "        leader_term_name_df.loc[index, 'term_end'] = dt(leader_term_name_df.loc[index, 'end_year'], leader_term_name_df.loc[index, 'end_month'], 28)\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "100%|██████████| 560/560 [00:00<00:00, 1263.64it/s]\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "source": [
                "# MATCHING TERM-WINDOWS (term length +/- 6 months) WITH ECONOMIST ISSUE DATES\r\n",
                "# We then pair this with the dat_index_reference.csv to figure out which chunks of formatted_compiled_articles.csv we should read\r\n",
                "six_month_margin = relativedelta(months = 6)\r\n",
                "\r\n",
                "leaders_windows_indices_df = leader_term_name_df\r\n",
                "\r\n",
                "# Constructing the 6 month window around the term start\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.assign(leader_aprox_starts = lambda df: df.term_start - six_month_margin)\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.assign(leader_aprox_ends = lambda df: df.term_end + six_month_margin)\r\n",
                "\r\n",
                "\r\n",
                "# Merge window-start and window-end dates with the \"nearest\" (FORWARD OR BACKWARD) Economist issue date.\r\n",
                "# Read in that issue's starting row/index\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.sort_values(by=['leader_aprox_starts'])\r\n",
                "leaders_windows_indices_df = pd.merge_asof(left=leaders_windows_indices_df, right=date_index_df, left_on='leader_aprox_starts', right_on='date_issue', direction='nearest')\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.drop(columns=['issue_ending_row'])\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.rename(columns={'date_issue' : 'date_start_issue'})\r\n",
                "\r\n",
                "\r\n",
                "# Merge window-start and window-end dates with the \"nearest\" (FORWARD OR BACKWARD) Economist issue date.\r\n",
                "# Read in that issue's ending row/index\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.sort_values(by=['leader_aprox_ends'])\r\n",
                "leaders_windows_indices_df = pd.merge_asof(left=leaders_windows_indices_df, right=date_index_df, left_on='leader_aprox_ends', right_on='date_issue', direction='nearest')\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.drop(columns=['issue_starting_row_y'])\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.rename(columns={'date_issue': 'date_end_issue', 'issue_starting_row_x': 'issue_starting_row'})\r\n",
                "\r\n",
                "\r\n",
                "# Generating number of rows that need to be read-in from the formatted_compiled_articles.csv file\r\n",
                "leaders_windows_indices_df = leaders_windows_indices_df.assign(nrows = lambda df: df.issue_ending_row - df.issue_starting_row - 1)\r\n",
                "\r\n",
                "# Add in titles and adjectives\r\n",
                "adjectives_df = pd.read_csv('national_titles_adjectives.csv').drop(columns=['country'])\r\n",
                "leaders_windows_indices_df = pd.merge(leaders_windows_indices_df, adjectives_df, how='left', on='ccode')\r\n",
                "\r\n",
                "\r\n",
                "# leaders_windows_indices_df.head(1)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "source": [
                "## SOME DOCUMENTATION\r\n",
                "# https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\r\n",
                "# https://github.com/seatgeek/fuzzywuzzy\r\n",
                "def leaderAliasGenerator(leader_observation):\r\n",
                "    row = leader_observation\r\n",
                "    full_name_tuple = (row.econ_style_first, row.econ_style_last)\r\n",
                "    full_name = str.title(' '.join(full_name_tuple))\r\n",
                "\r\n",
                "    title_last_pref_tuple = (row.hos_title, row.econ_style_last)\r\n",
                "    title_last_pref = str.title(' '.join(title_last_pref_tuple))\r\n",
                "\r\n",
                "    # Adding gendered honorifics\r\n",
                "    if row.gender == 1:\r\n",
                "        hon_list = []\r\n",
                "        hon = \"Mr\"\r\n",
                "        hon_last = str.title(' '.join([hon, row.econ_style_last]))\r\n",
                "        hon_list.append(hon_last)\r\n",
                "    else:\r\n",
                "        hon_1 = \"Ms\"\r\n",
                "        hon_last_1 = str.title(' '.join([hon_1, row.econ_style_last]))\r\n",
                "        hon_2 = \"Miss\"\r\n",
                "        hon_last_2 = str.title(' '.join([hon_2, row.econ_style_last]))\r\n",
                "        hon_3 = \"Mrs\"\r\n",
                "        hon_last_3 = str.title(' '.join([hon_3, row.econ_style_last]))\r\n",
                "\r\n",
                "    choices = [full_name, title_last_pref] + hon_list\r\n",
                "\r\n",
                "\r\n",
                "    #Adding other/type of honorific if necessary/available\r\n",
                "    if type(row.hos_title_other) != float:\r\n",
                "        title_last_alt_tuple = (row.hos_title_other, row.econ_style_last)\r\n",
                "        title_last_alt = str.title(' '.join(title_last_alt_tuple))\r\n",
                "        \r\n",
                "        choices.insert(2, title_last_alt)\r\n",
                "        \r\n",
                "    return choices\r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "source": [
                "# Load in the coreference resolution tool/object\r\n",
                "predictor = Predictor.from_path('https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz')\r\n",
                "nlp = spacy.load('en_core_web_sm')\r\n"
            ],
            "outputs": [
                {
                    "output_type": "stream",
                    "name": "stderr",
                    "text": [
                        "error loading _jsonnet (this is expected on Windows), treating C:\\Users\\Joshua\\AppData\\Local\\Temp\\tmpf0c5ehfg\\config.json as plain json\n",
                        "Some weights of BertModel were not initialized from the model checkpoint at SpanBERT/spanbert-large-cased and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                }
            ],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "LEADER_BATCH_SIZE  = 2\r\n",
                "\r\n",
                "# Check leaders we have already done\r\n",
                "resolved_leaders_df = pd.read_csv('resolved_leader_tracker.csv')\r\n",
                "\r\n",
                "non_resolved_leaders_df = pd.merge(leaders_windows_indices_df, resolved_leaders_df, how='outer', on='leadid',indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])\r\n",
                "\r\n",
                "\r\n",
                "# leader_batch_sample = non_resolved_leaders_df.sample(n=LEADER_BATCH_SIZE)\r\n",
                "# leader_batch_sample = non_resolved_leaders_df.loc[non_resolved_leaders_df.leadid in [\"A2.9-8200\", \"A2.9-4234\"]]\r\n",
                "leader_batch_sample = non_resolved_leaders_df.loc[non_resolved_leaders_df.leadid == \"A2.9-4234\"]\r\n",
                "\r\n",
                "# Run this loop for every leader in this batch\r\n",
                "for index in leader_batch_sample.index.tolist():\r\n",
                "    \r\n",
                "    term_info = leader_batch_sample.loc[index, :]\r\n",
                "    name = term_info.econ_style_last.replace('.','')\r\n",
                "    name = name.replace(' ', '')\r\n",
                "    leaderid = term_info.leadid.replace('.','')\r\n",
                "    leader_gender_hon = term_info.gender\r\n",
                "    start_at_0 = True if term_info.issue_starting_row==0 else False\r\n",
                "\r\n",
                "    # These are the columns that we are going to get by default (make sure to title the new df with these incase we don't read row 0 of the .csv)\r\n",
                "    col_names = ['date', 'link', 'text']\r\n",
                "\r\n",
                "    # Read in only the subset of the massive formatted_compiled_articles that pertain to a 6-month window around this leader's term\r\n",
                "    if start_at_0:\r\n",
                "        chunks = pd.read_csv('formatted_compiled_articles.csv', chunksize=1000, parse_dates=['date'], date_parser=custom_date_parser, nrows=term_info.nrows)\r\n",
                "        df = pd.concat(chunks)\r\n",
                "    else:\r\n",
                "        chunks = pd.read_csv('formatted_compiled_articles.csv', names = col_names, chunksize=1000, parse_dates=['date'], date_parser=custom_date_parser, skiprows=term_info.issue_starting_row +1, nrows=term_info.nrows)\r\n",
                "        df = pd.concat(chunks)\r\n",
                "\r\n",
                "    # Add some extra meta-data to this article-level observation\r\n",
                "    df['ccode'] = term_info.ccode\r\n",
                "    df['country'] = term_info.country\r\n",
                "    df['resolved_text'] = ''\r\n",
                "    df['pre_in_post_term'] = ''\r\n",
                "    df[name] = False\r\n",
                "\r\n",
                "    # Coerce some of the df[name] observations to True(using a fuzzy match) to identify the subset of all articles in this term +/-6 month window that actually mention the leader\r\n",
                "    leader_alias_choices = leaderAliasGenerator(term_info)\r\n",
                "    print(leader_alias_choices)\r\n",
                "    for leader_dummy_idx in tqdm(df.index.tolist(), desc='FUZZY MATCH/SEARCH'):\r\n",
                "        string_to_search = df.loc[leader_dummy_idx, 'text']\r\n",
                "        df.loc[leader_dummy_idx, name] = fuzzy_leader_search(string_to_search, leader_alias_choices)\r\n",
                "\r\n",
                "    df = df[df[name] == True]\r\n",
                "\r\n",
                "    print(\"number of articles identified for \" + name + \": \" + str(len(df)))\r\n",
                "\r\n",
                "    # Do coreference resolution on that subset of articles that mention the leader\r\n",
                "    for row_index in tqdm(df.index.tolist()):\r\n",
                "        try:\r\n",
                "            pre_resolve_text = df.loc[row_index, 'text']\r\n",
                "            pred_obj = predictor.predict(document = pre_resolve_text)\r\n",
                "\r\n",
                "            spacy_df = pd.DataFrame([], columns = ['text', 'lemma', 'pos', 'tag', 'dep'])\r\n",
                "\r\n",
                "            document_clusters = pred_obj['clusters']    \r\n",
                "            document_list = pred_obj['document']\r\n",
                "            spacy_doc = nlp(pre_resolve_text) \r\n",
                "\r\n",
                "            leader_name = name.title()\r\n",
                "\r\n",
                "            for token in spacy_doc:\r\n",
                "                tok_observation = [token.text, token.lemma_, token.pos_, token.tag_, token.dep_]\r\n",
                "                spacy_df.loc[len(spacy_df)] = tok_observation\r\n",
                "\r\n",
                "            cluster_of_interest = identifyMainCluster(document_clusters, leader_name, document_list)\r\n",
                "\r\n",
                "            replaced = replaceCoref(leader_name, document_clusters, cluster_of_interest, document_list, spacy_df)\r\n",
                "\r\n",
                "            df.loc[row_index, 'resolved_text'] = ' '.join(replaced)\r\n",
                "\r\n",
                "        except:\r\n",
                "            df.loc[row_index, 'resolved_text'] = df.loc[row_index, 'text']\r\n",
                "            continue\r\n",
                "\r\n",
                "        df.loc[row_index, 'pre_in_post_term'] = termLimitChecker(df.loc[row_index, 'date'], term_info.term_start, term_info.term_end)\r\n",
                "    \r\n",
                "    \r\n",
                "    df.to_csv('leader_resolved/' + name + '_resolved' + leaderid + '.csv', index=False)\r\n",
                "    \r\n",
                "    resolved_leaders_df.loc[len(resolved_leaders_df)]=[term_info.leader_x, term_info.leadid, 'RESOLVED']\r\n",
                "\r\n",
                "    resolved_leaders_df.to_csv('resolved_leader_tracker.csv', index=False)"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 16,
            "source": [
                "def fuzzy_leader_search(plain_text, choices): \r\n",
                "    honorifics = [\"Mr\", \"Ms\", \"Miss\", \"Mrs\"]\r\n",
                "    topSetMatch = process.extract(plain_text, choices, scorer=fuzz.token_set_ratio)\r\n",
                "\r\n",
                "    for alias in topSetMatch:\r\n",
                "        # IF WE ARE CONSIDERING Mr/Ms _____ AS THE CANDIDATE HONORIFIC RAISE THE THRESHOLD FOR MATCH TO 100 (BASICALLY PERFECT MATCH)\r\n",
                "        if alias[0].split()[0] in honorifics:\r\n",
                "            if alias[1] == 100:\r\n",
                "                return True\r\n",
                "            else:\r\n",
                "                continue\r\n",
                "        # WE ARE CONSIDERING A FULL NAME OR A TILE NAME Like President ____\r\n",
                "        else:\r\n",
                "            if alias[1] >= 90:\r\n",
                "                return True\r\n",
                "\r\n",
                "    return False \r\n"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "source": [
                "def identifyMainCluster(clusters, name, document):\r\n",
                "    clusters = clusters\r\n",
                "\r\n",
                "    for cluster in clusters:\r\n",
                "        for index_pair in cluster:\r\n",
                "            start_index = index_pair[0]\r\n",
                "            stop_index = index_pair[1]+1\r\n",
                "            cluster_as_string = ' '.join(document[start_index:stop_index])\r\n",
                "\r\n",
                "            if (cluster_as_string == name) or (cluster_as_string == name + \"\\'s\"):\r\n",
                "                cluster_index = clusters.index(cluster)\r\n",
                "                return cluster_index"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "source": [
                "def replaceCoref(name, clusters, cluster_interest_index, document, tokenized_df):\r\n",
                "    name_pos = name + \"\\'s\"\r\n",
                "\r\n",
                "    base_cluster = clusters[cluster_interest_index]\r\n",
                "    rev_cluster  = sorted(base_cluster, key = lambda x: x[0], reverse = True)\r\n",
                "\r\n",
                "    new_doc_list = document\r\n",
                "\r\n",
                "    df = tokenized_df\r\n",
                "    for index_pair in rev_cluster:\r\n",
                "        start_index = index_pair[0]\r\n",
                "        stop_index = index_pair[1]+1\r\n",
                "\r\n",
                "        # Replacing one element (i.e. pronouns/possessives)\r\n",
                "        if start_index == stop_index:\r\n",
                "            if containsPossessive(df, start_index, stop_index) == True:\r\n",
                "                new_doc_list[start_index:stop_index] = name_pos\r\n",
                "            else:\r\n",
                "                new_doc_list[start_index:stop_index] = name\r\n",
                "            new_doc_list.pop(start_index+2)\r\n",
                "        # Replacing two or more elements\r\n",
                "        else:\r\n",
                "            if containsPossessive(df, start_index, stop_index) == True:\r\n",
                "                new_doc_list[start_index:stop_index] = name_pos\r\n",
                "            else:\r\n",
                "                new_doc_list[start_index:stop_index] = name\r\n",
                "\r\n",
                "    return new_doc_list"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": 14,
            "source": [
                "def containsPossessive(df, start_index, stop_index):\r\n",
                "    possessive_tags = ['POS', 'PRP$']\r\n",
                "    cluster_tags = df['tag'].iloc[start_index:stop_index]\r\n",
                "    # print(start_index, stop_index)\r\n",
                "    # print(df['text'].iloc[start_index:stop_index])\r\n",
                "    # print(set(possessive_tags))\r\n",
                "    # .isdisjoint returns true if the two sets are disjoin (i.e. do not have any elements interesecting (i.e. does not contain a POS or PRP$)). So negate to return True if \"NOT DISJOINT\" (CONTAINS POS/PRP$)\r\n",
                "    if not set(possessive_tags).isdisjoint(set(cluster_tags)):\r\n",
                "        return True\r\n",
                "    else:\r\n",
                "        return False"
            ],
            "outputs": [],
            "metadata": {}
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "source": [
                "def termLimitChecker(date, start, end):\r\n",
                "\r\n",
                "    if start <= date <= end: \r\n",
                "        return \"IN TERM\"\r\n",
                "    elif date < start:\r\n",
                "        return \"PRE TERM\"\r\n",
                "    else:\r\n",
                "        return \"POST TERM\""
            ],
            "outputs": [],
            "metadata": {}
        }
    ],
    "metadata": {
        "orig_nbformat": 4,
        "language_info": {
            "name": "python",
            "version": "3.9.1",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        },
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.9.1 64-bit"
        },
        "interpreter": {
            "hash": "ac59ebe37160ed0dfa835113d9b8498d9f09ceb179beaac4002f036b9467c963"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}