{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\r\n",
    "from allennlp.predictors.predictor import Predictor\r\n",
    "from datetime import datetime as dt\r\n",
    "from dateutil.relativedelta import *\r\n",
    "from tqdm import tqdm\r\n",
    "from fuzzywuzzy import fuzz\r\n",
    "from fuzzywuzzy import process\r\n",
    "import os\r\n",
    "import allennlp_models.tagging\r\n",
    "import spacy\r\n",
    "import re\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Load in the coreference resolution tool/object\r\n",
    "predictor = Predictor.from_path('https://storage.googleapis.com/allennlp-public-models/coref-spanbert-large-2021.03.10.tar.gz')\r\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "## SOME DOCUMENTATION\r\n",
    "# https://chairnerd.seatgeek.com/fuzzywuzzy-fuzzy-string-matching-in-python/\r\n",
    "# https://github.com/seatgeek/fuzzywuzzy\r\n",
    "def leaderAliasGenerator(leader_observation):\r\n",
    "    row = leader_observation\r\n",
    "    full_name_tuple = (row.econ_style_first, row.econ_style_last)\r\n",
    "    full_name = str.title(' '.join(full_name_tuple))\r\n",
    "\r\n",
    "    title_last_pref_tuple = (row.hos_title, row.econ_style_last)\r\n",
    "    title_last_pref = str.title(' '.join(title_last_pref_tuple))\r\n",
    "\r\n",
    "\r\n",
    "    \r\n",
    "    # Adding gendered honorifics\r\n",
    "    if row.gender == 1:\r\n",
    "        hon_list = []\r\n",
    "        hon = \"Mr\"\r\n",
    "        hon_last = str.title(' '.join([hon, row.econ_style_last]))\r\n",
    "        hon_list.append(hon_last)\r\n",
    "    else:\r\n",
    "        hon_1 = \"Ms\"\r\n",
    "        hon_last_1 = str.title(' '.join([hon_1, row.econ_style_last]))\r\n",
    "        hon_2 = \"Miss\"\r\n",
    "        hon_last_2 = str.title(' '.join([hon_2, row.econ_style_last]))\r\n",
    "        hon_3 = \"Mrs\"\r\n",
    "        hon_last_3 = str.title(' '.join([hon_3, row.econ_style_last]))\r\n",
    "\r\n",
    "\r\n",
    "    choices = [full_name, title_last_pref] + hon_list\r\n",
    "\r\n",
    "\r\n",
    "    #Adding other/type of honorific if necessary/available\r\n",
    "    if type(row.hos_title_other) != float:\r\n",
    "        title_last_alt_tuple = (row.hos_title_other, row.econ_style_last)\r\n",
    "        title_last_alt = str.title(' '.join(title_last_alt_tuple))\r\n",
    "        \r\n",
    "        choices.insert(2, title_last_alt)\r\n",
    "\r\n",
    "    # Adding Economist-style alias if necessary/available\r\n",
    "    if type(row.econ_style_alias) != float:\r\n",
    "        alias = row.econ_style_alias\r\n",
    "        econ_alias = str.title(alias)\r\n",
    "\r\n",
    "        choices.insert(2, econ_alias)\r\n",
    "\r\n",
    "    return choices\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def fuzzy_leader_search(plain_text, choices): \r\n",
    "    honorifics = [\"Mr\", \"Ms\", \"Miss\", \"Mrs\"]\r\n",
    "    topSetMatch = process.extract(plain_text, choices, scorer=fuzz.token_set_ratio)\r\n",
    "\r\n",
    "    for alias in topSetMatch:\r\n",
    "        # IF WE ARE CONSIDERING Mr/Ms _____ AS THE CANDIDATE HONORIFIC RAISE THE THRESHOLD FOR MATCH TO 100 (BASICALLY PERFECT MATCH)\r\n",
    "        if alias[0].split()[0] in honorifics:\r\n",
    "            if alias[1] == 100:\r\n",
    "                return True\r\n",
    "            else:\r\n",
    "                continue\r\n",
    "        # WE ARE CONSIDERING A FULL NAME OR A TILE NAME Like President ____\r\n",
    "        else:\r\n",
    "            if alias[1] >= 90:\r\n",
    "                return True\r\n",
    "\r\n",
    "    return False \r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def replaceCoref(name, clusters, cluster_interest_index, document, tokenized_df):\r\n",
    "    name_pos = name + \"\\'s\"\r\n",
    "    name_pos = name_pos.split()\r\n",
    "    name = name.split()\r\n",
    "\r\n",
    "    # print(cluster_interest_index)\r\n",
    "    base_cluster = clusters[cluster_interest_index]\r\n",
    "    # print(base_cluster)\r\n",
    "    \r\n",
    "    #Coreferences are replaced in reverse order so that we as we replace references that occur towards the end of the article we don't have to worry about how new string/list length is affected. That is, the indices identified in base_cluster are still valid.\r\n",
    "    rev_cluster  = sorted(base_cluster, key = lambda x: x[0], reverse = True)\r\n",
    "    new_doc_list = document\r\n",
    "\r\n",
    "    df = tokenized_df\r\n",
    "    # print(\"REPLACING WITH: \" + str(name) + \" (or possessive)\")\r\n",
    "    for index_pair in rev_cluster:\r\n",
    "        start_index = index_pair[0]\r\n",
    "        stop_index = index_pair[1]+1\r\n",
    "        # print(\"REPLACING: \" + str(new_doc_list[start_index:stop_index]))\r\n",
    "\r\n",
    "        # Replacing one element (i.e. pronouns/possessives)\r\n",
    "        if start_index == stop_index:\r\n",
    "            if containsPossessive(df, start_index, stop_index) == True:\r\n",
    "                new_doc_list[start_index:stop_index] = name_pos\r\n",
    "            else:\r\n",
    "                new_doc_list[start_index:stop_index] = name\r\n",
    "            new_doc_list.pop(start_index+2)\r\n",
    "        # Replacing two or more elements\r\n",
    "        else:\r\n",
    "            if containsPossessive(df, start_index, stop_index) == True:\r\n",
    "                new_doc_list[start_index:stop_index] = name_pos\r\n",
    "            else:\r\n",
    "                new_doc_list[start_index:stop_index] = name\r\n",
    "\r\n",
    "    # print(new_doc_list)\r\n",
    "    return new_doc_list"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def containsPossessive(df, start_index, stop_index):\r\n",
    "    possessive_tags = ['POS', 'PRP$']\r\n",
    "    cluster_tags = df['tag'].iloc[start_index:stop_index]\r\n",
    "    # .isdisjoint returns true if the two sets are disjoin (i.e. do not have any elements interesecting (i.e. does not contain a POS or PRP$)). So negate to return True if \"NOT DISJOINT\" (CONTAINS POS/PRP$)\r\n",
    "    if not set(possessive_tags).isdisjoint(set(cluster_tags)):\r\n",
    "        return True\r\n",
    "    else:\r\n",
    "        return False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def identifyMainCluster(clusters, name, document):\r\n",
    "    clusters = clusters\r\n",
    "    aliases = name\r\n",
    "\r\n",
    "\r\n",
    "    for cluster in clusters:\r\n",
    "        for index_pair in cluster:\r\n",
    "            start_index = index_pair[0]\r\n",
    "            stop_index = index_pair[1]+1\r\n",
    "            cluster_as_string = ' '.join(document[start_index:stop_index])\r\n",
    "\r\n",
    "            top_set_match = process.extract(cluster_as_string, aliases)\r\n",
    "            fuzz_match_results = zip(*top_set_match)\r\n",
    "            fuzz_match_res_unzipped = list(fuzz_match_results)\r\n",
    "            scores = fuzz_match_res_unzipped[1]\r\n",
    "\r\n",
    "            if max(scores) >= 90:\r\n",
    "                cluster_index = clusters.index(cluster)\r\n",
    "                # print(\"CLUSTER AS STRING: \" + cluster_as_string)\r\n",
    "                # print(\"FUZZY MATCH SCORES: \" + str(top_set_match))\r\n",
    "                # print(\"CLUSTER: \" + str(cluster))\r\n",
    "                # print(\"INDEX OF THAT CLUSTER\" + str(cluster_index))\r\n",
    "                return cluster_index\r\n",
    "\r\n",
    "    # There may be instances in which the leader is mentioned but not in a fashion that is sufficiently important to be picked up as an entity by the model. These are usually in instances where the leader is only an object in the sentence and never becomes a subject. In those instances no replacement takes place.\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def termLimitChecker(date, start, end):\r\n",
    "\r\n",
    "    if start <= date <= end: \r\n",
    "        return \"IN TERM\"\r\n",
    "    elif date < start:\r\n",
    "        return \"PRE TERM\"\r\n",
    "    else:\r\n",
    "        return \"POST TERM\""
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "date_index_reference = 'issue_sheet_index.csv'\r\n",
    "# chunksize = 1000\r\n",
    "date_format = '%d-%b-%y'\r\n",
    "custom_date_parser = lambda x: dt.strptime(x, date_format)\r\n",
    "\r\n",
    "formatted_csv_path = '../00_The_Economist_Scraper/formatted_csvs/'\r\n",
    "formatted_csv_stub = 'formatted_sub_article_text'\r\n",
    "suffix = '.csv'\r\n",
    "\r\n",
    "if not os.path.isfile(date_index_reference):\r\n",
    "    print('date index needs to be instantiated, proceeding to do that')\r\n",
    "    #Initialize the index df\r\n",
    "    index_df = pd.DataFrame([], columns=['date_issue', 'sheet_path', 'sheet_num'])    \r\n",
    "\r\n",
    "    # GOING THROUGH EACH FORMATTED CSV\r\n",
    "    for sub_csv_index in tqdm(range(0,124)):\r\n",
    "        read_path = formatted_csv_path + formatted_csv_stub + str(sub_csv_index) + suffix\r\n",
    "        temp_df =  pd.read_csv(read_path, parse_dates=['date'], date_parser=custom_date_parser)\r\n",
    "\r\n",
    "        # Identifying unique dates\r\n",
    "        temp_df = temp_df.drop_duplicates(subset=['date'], keep='first')\r\n",
    "        temp_df = temp_df['date']\r\n",
    "\r\n",
    "        temp_index = pd.DataFrame([], columns=['date_issue', 'sheet_path', 'sheet_num'])\r\n",
    "\r\n",
    "        # Pulling out the info of interest \r\n",
    "        for issue_row in temp_df.index.tolist():\r\n",
    "            row_obs = temp_df.loc[issue_row]\r\n",
    "            # recall that temp_df is now just a single column of 'date\r\n",
    "            temp_index.loc[issue_row, 'date_issue'] = temp_df.loc[issue_row].date()\r\n",
    "            temp_index.loc[issue_row, 'sheet_path'] = formatted_csv_path + formatted_csv_stub + str(sub_csv_index) + suffix\r\n",
    "            temp_index.loc[issue_row, 'sheet_num'] = sub_csv_index\r\n",
    "        ### REMEMBER THAT A DATE MIGHT APPEAR IN/CROSS OVER MORE THAN ONE SHEET\r\n",
    "        # Need to concat and reset indices because first instances of different issues may appear on the same row (i.e. \"index\" of temp_df) of different sheets. Need to do this to avoid conflicts\r\n",
    "        index_df = pd.concat([index_df, temp_index], ignore_index=True)\r\n",
    "        index_df.reset_index(drop=True, inplace=True)\r\n",
    "        index_df['date_issue'] = pd.to_datetime(index_df['date_issue'])\r\n",
    "\r\n",
    "    index_df.to_csv(date_index_reference, index=False)\r\n",
    "        \r\n",
    "else:\r\n",
    "    print('Issues are already indexed for their dates, carry on')\r\n",
    "    index_df = pd.read_csv(date_index_reference)\r\n",
    "    index_df['date_issue'] = pd.to_datetime(index_df['date_issue'])\r\n",
    "\r\n",
    "print(index_df.head(5))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "### READING IN LEADER TERM DATA\r\n",
    "leader_term_name_df = pd.read_csv('all_leaders_econ_styling.csv', encoding='latin1')\r\n",
    "leader_term_name_df = leader_term_name_df.drop_duplicates(subset='leadid')\r\n",
    "\r\n",
    "\r\n",
    "leader_term_name_df['term_start'] = dt.now\r\n",
    "leader_term_name_df['term_end'] =dt.now\r\n",
    "\r\n",
    "# leader_term_name_df\r\n",
    "for index in tqdm(leader_term_name_df.index.tolist()):\r\n",
    "    # FORMAT: start = dt(year, month, day)\r\n",
    "    leader_term_name_df.loc[index, 'term_start'] = dt(leader_term_name_df.loc[index, 'start_year'], leader_term_name_df.loc[index, 'start_month'], leader_term_name_df.loc[index, 'start_date'])\r\n",
    "    \r\n",
    "    # Push to the end of the month as far as possible (accounting for leap years)\r\n",
    "    # Note that we are pushing the end of the term out 6 months anyway (in case we miss an issue printed on the 29th-31st)\r\n",
    "    leader_term_name_df.loc[index, 'term_end'] = dt(leader_term_name_df.loc[index, 'end_year'], leader_term_name_df.loc[index, 'end_month'], 28)\r\n",
    "\r\n",
    "# print(leader_term_name_df.head(5))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# MATCHING TERM-WINDOWS (term length +/- 6 months) WITH ECONOMIST ISSUE DATES\r\n",
    "# We then pair this with the dat_index_reference.csv to figure out which chunks of formatted_compiled_articles.csv we should read\r\n",
    "six_month_margin = relativedelta(months = 6)\r\n",
    "\r\n",
    "leaders_windows_indices_df = leader_term_name_df\r\n",
    "\r\n",
    "# Constructing the 6 month window around the term start\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.assign(leader_aprox_starts = lambda df: df.term_start - six_month_margin)\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.assign(leader_aprox_ends = lambda df: df.term_end + six_month_margin)\r\n",
    "leaders_windows_indices_df['leader_aprox_starts'] = pd.to_datetime(leaders_windows_indices_df['leader_aprox_starts'])\r\n",
    "\r\n",
    "# Merge window-start and window-end dates with the \"nearest\" (FORWARD OR BACKWARD) Economist issue date.\r\n",
    "# Read in that issue's starting row/index\r\n",
    "# PRE-TERM\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.sort_values(by=['leader_aprox_starts'])\r\n",
    "leaders_windows_indices_df = pd.merge_asof(left=leaders_windows_indices_df, right=index_df, left_on='leader_aprox_starts', right_on='date_issue', direction='nearest')\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.rename(columns={'date_issue' : 'date_start_issue', 'sheet_path' : 'start_sheet_path', 'sheet_num' : 'start_sheet_num'})\r\n",
    "\r\n",
    "# POST-TERM\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.sort_values(by=['leader_aprox_ends'])\r\n",
    "leaders_windows_indices_df = pd.merge_asof(left=leaders_windows_indices_df, right=index_df, left_on='leader_aprox_ends', right_on='date_issue', direction='nearest')\r\n",
    "leaders_windows_indices_df = leaders_windows_indices_df.rename(columns={'date_issue': 'date_end_issue', 'sheet_path': 'end_sheet_path', 'sheet_num' : 'end_sheet_num'})\r\n",
    "\r\n",
    "################################################\r\n",
    "# Add in titles and adjectives\r\n",
    "################################################\r\n",
    "adjectives_df = pd.read_csv('national_titles_adjectives.csv').drop(columns=['country'])\r\n",
    "leaders_windows_indices_df = pd.merge(leaders_windows_indices_df, adjectives_df, how='left', on='ccode')\r\n",
    "\r\n",
    "\r\n",
    "print(len(leader_term_name_df))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "LEADER_BATCH_SIZE  = 2\r\n",
    "\r\n",
    "# Check leaders we have already done\r\n",
    "resolved_leaders_df = pd.read_csv('resolved_leader_tracker_temp.csv')\r\n",
    "\r\n",
    "non_resolved_leaders_df = pd.merge(leaders_windows_indices_df, resolved_leaders_df, how='outer', on='leadid',indicator=True).query('_merge == \"left_only\"').drop(columns=['_merge'])\r\n",
    "\r\n",
    "\r\n",
    "# leader_batch_sample = non_resolved_leaders_df.sample(n=LEADER_BATCH_SIZE)\r\n",
    "# leader_batch_sample = non_resolved_leaders_df.loc[non_resolved_leaders_df.leadid in [\"A2.9-8200\", \"A2.9-4234\"]]\r\n",
    "leader_batch_sample = non_resolved_leaders_df.loc[non_resolved_leaders_df.leadid == \"A2.9-4231\"]\r\n",
    "\r\n",
    "# Run this loop for every leader in this batch\r\n",
    "for index in leader_batch_sample.index.tolist():\r\n",
    "    \r\n",
    "    term_info = leader_batch_sample.loc[index, :]\r\n",
    "    name = term_info.econ_style_last.replace('.','')\r\n",
    "    name = name.replace(' ', '')\r\n",
    "    leaderid = term_info.leadid.replace('.','')\r\n",
    "    leader_gender_hon = term_info.gender\r\n",
    "    start_sheet = term_info.start_sheet_num\r\n",
    "    end_sheet = term_info.end_sheet_num\r\n",
    "    # start_at_0 = True if term_info.issue_starting_row==0 else False\r\n",
    "\r\n",
    "    # Instantiate a list which will contain DFs for future concatenation\r\n",
    "    chunk_list = []\r\n",
    "\r\n",
    "    # These are the columns that we are going to get by default (make sure to title the new df with these incase we don't read row 0 of the .csv)\r\n",
    "    # col_names = ['date', 'link', 'text']\r\n",
    "    print(start_sheet, end_sheet)\r\n",
    "    print('loading ' + str(end_sheet - start_sheet + 1) + ' sheets')\r\n",
    "\r\n",
    "\r\n",
    "    ##### NEED TO FIX THIS LINE RANGE\r\n",
    "    for sheet_num in range(start_sheet, end_sheet+1):\r\n",
    "        chunk_path = formatted_csv_path + formatted_csv_stub + str(sheet_num) + suffix\r\n",
    "        chunk_df = pd.read_csv(chunk_path, parse_dates=['date'], date_parser=custom_date_parser)\r\n",
    "        chunk_list.append(chunk_df)\r\n",
    "\r\n",
    "    df = pd.concat(chunk_list, ignore_index=True)\r\n",
    "\r\n",
    "    # Add some extra meta-data to this article-level observation\r\n",
    "    df['ccode'] = term_info.ccode\r\n",
    "    df['country'] = term_info.country\r\n",
    "    df['resolved_text'] = ''\r\n",
    "    df['pre_in_post_term'] = ''\r\n",
    "    df['coreference_resolved_ind'] = False\r\n",
    "    df[name] = False\r\n",
    "\r\n",
    "    # Coerce some of the df[name] observations to True(using a fuzzy match) to identify the subset of all articles in this term +/-6 month window that actually mention the leader\r\n",
    "    leader_alias_choices = leaderAliasGenerator(term_info)\r\n",
    "    print('fuzzy searching on {} rows'.format(len(df)))\r\n",
    "    print(leader_alias_choices)\r\n",
    "    for leader_dummy_idx in tqdm(df.index.tolist(), desc='FUZZY MATCH/SEARCH'):\r\n",
    "        string_to_search = df.loc[leader_dummy_idx, 'text']\r\n",
    "        df.loc[leader_dummy_idx, name] = fuzzy_leader_search(string_to_search, leader_alias_choices)\r\n",
    "\r\n",
    "    df = df[df[name] == True]\r\n",
    "    df = df.head(8)\r\n",
    "\r\n",
    "\r\n",
    "    print(\"number of articles identified for \" + name + \": \" + str(len(df)))\r\n",
    "\r\n",
    "\r\n",
    "    startTime = dt.now()\r\n",
    "    # Do coreference resolution on that subset of articles that mention the leader\r\n",
    "    for row_index in tqdm(df.index.tolist(), desc=name):\r\n",
    "        try:\r\n",
    "            pre_resolve_text = df.loc[row_index, 'text']\r\n",
    "            pred_obj = predictor.predict(document = pre_resolve_text)\r\n",
    "\r\n",
    "            spacy_df = pd.DataFrame([], columns = ['text', 'lemma', 'pos', 'tag', 'dep'])\r\n",
    "\r\n",
    "            document_clusters = pred_obj['clusters']    \r\n",
    "            document_list = pred_obj['document']\r\n",
    "            spacy_doc = nlp(pre_resolve_text) \r\n",
    "\r\n",
    "            leader_name = leader_alias_choices[0]\r\n",
    "\r\n",
    "            for token in spacy_doc:\r\n",
    "                tok_observation = [token.text, token.lemma_, token.pos_, token.tag_, token.dep_]\r\n",
    "                spacy_df.loc[len(spacy_df)] = tok_observation\r\n",
    "\r\n",
    "            cluster_of_interest = identifyMainCluster(document_clusters, leader_alias_choices, document_list)\r\n",
    "            print(leader_name)\r\n",
    "\r\n",
    "            replaced = replaceCoref(leader_name, document_clusters, cluster_of_interest, document_list, spacy_df)\r\n",
    "\r\n",
    "            df.loc[row_index, 'resolved_text'] = ' '.join(replaced)\r\n",
    "            df.loc[row_index, 'coreference_resolved_ind'] = True\r\n",
    "            # print('successfully replaced coreferences')\r\n",
    "\r\n",
    "        except Exception as e:\r\n",
    "            print('excepting')\r\n",
    "            print(e)\r\n",
    "            df.loc[row_index, 'resolved_text'] = df.loc[row_index, 'text']\r\n",
    "            continue\r\n",
    "\r\n",
    "        df.loc[row_index, 'pre_in_post_term'] = termLimitChecker(df.loc[row_index, 'date'], term_info.term_start, term_info.term_end)\r\n",
    "    \r\n",
    "    \r\n",
    "    df.to_csv('leader_resolved/' + name + '_resolved' + leaderid + '.csv', index=False)\r\n",
    "    print('Time to coresolve XXX observations: {}'.format(dt.now() - startTime))\r\n",
    "    resolved_leaders_df.loc[len(resolved_leaders_df)]=[term_info.leader_x, term_info.leadid, 'RESOLVED']\r\n",
    "\r\n",
    "    resolved_leaders_df.to_csv('resolved_leader_tracker_temp.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.7",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.7 64-bit ('pandastest': conda)"
  },
  "interpreter": {
   "hash": "c5e5d7fb3d81e909d19c7041dfc7fdf19de476fcee5121b32a71cede18f9d3ba"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}